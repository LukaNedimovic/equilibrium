{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7052af-21e1-4863-a121-a3f90e963aa7",
   "metadata": {},
   "source": [
    "<h1><center> Model Notebook </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627365b5-ab4e-463d-8a04-417f7e0594dc",
   "metadata": {},
   "source": [
    "<h2> <center> 1. Data Preparation </center> </h2>\n",
    "<h3> <center> 1.1. Data Cleaning </center></h3>\n",
    "Dataset used can be found on Kaggle: <a href=\"https://www.kaggle.com/datasets/fabiochiusano/medium-articles\"> Click here to view the dataset </a> <br/>\n",
    "It contains 190k+ Medium articles, but for our training purposes, only first <i> <b> N_ROWS = 1000 </b> </i> have been used.\n",
    "\n",
    "<b> Data Description </b> <br/>\n",
    "\n",
    "Each row in the data is a different article published on Medium. For each article, you have the following features: <br/>\r",
    "<ul>\n",
    "    <li> <b> title </b> <i>[string]</i>: The title of the article. </li>\n",
    "    <li> <b> text </b> <i>[string]</i>: The text content of the article. </li>\n",
    "    <li> <b> url </b> <i>[string]</i>: The URL associated to the article. </li>\n",
    "    <li> <b> authors </b> <i>[list of strings]</i>: The article authors. </li>\n",
    "    <li> <b> timestamp </b> <i>[string]</i>: The publication datetime of the article. </li>\n",
    "    <li> <b> tags </b> <i>[list of strings]</i>: List of tags associated to the article. </li>\n",
    "</ul>\n",
    "\n",
    "For our training purposes, only <b> tags </b> column is relevant - everything else contributes not to embedding creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374865b9-bfc3-4158-b24c-510293b037a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party Library Imports\n",
    "import pandas as pd                  # Data processing\n",
    "import numpy as np                   # Math\n",
    "from typing import List, Tuple, Dict # Type hinting\n",
    "import ast                           # Literal evaluation\n",
    "\n",
    "# Model creation - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Prediction making\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6acb1e-a0a5-41b4-811e-9d64b1b665e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded! Data shape: (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"..//..//..//data/medium_articles.csv\"\n",
    "\n",
    "N_ROWS = 1000\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, nrows=N_ROWS)\n",
    "print(f\"Data successfully loaded! Data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa42f09-2c2e-4b6c-bb95-ee234b90bb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0               Mental Note Vol. 24   \n",
       "1         Your Brain On Coronavirus   \n",
       "2                    Mind Your Nose   \n",
       "3          The 4 Purposes of Dreams   \n",
       "4  Surviving a Rod Through the Head   \n",
       "\n",
       "                                                text  \\\n",
       "0  Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1  Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2  Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3  Passionate about the synergy between science a...   \n",
       "4  You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "\n",
       "                                                 url                 authors  \\\n",
       "0  https://medium.com/invisible-illness/mental-no...            ['Ryan Fan']   \n",
       "1  https://medium.com/age-of-awareness/how-the-pa...       ['Simon Spichak']   \n",
       "2  https://medium.com/neodotlife/mind-your-nose-f...                      []   \n",
       "3  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n",
       "4  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2020-12-26 03:38:10.479000+00:00   \n",
       "1  2020-09-23 22:10:17.126000+00:00   \n",
       "2  2020-10-10 20:17:37.132000+00:00   \n",
       "3  2020-12-21 16:05:19.524000+00:00   \n",
       "4  2020-02-26 00:01:01.576000+00:00   \n",
       "\n",
       "                                                tags  \n",
       "0  ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1  ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2  ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3  ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4  ['Brain', 'Health', 'Development', 'Psychology...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # Show first 5 entries, for examples sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a5f53f8-0748-4afe-8175-1ce26930d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"..//..//..//data/articles_tag_1k.csv\", columns=[\"tags\"], index=False) # Save dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e00f91-88e1-497c-a7e7-97dddb3bd4af",
   "metadata": {},
   "source": [
    "<h3> <center> 1.2. Data Preparation </center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9855e2-4197-434f-92f9-63f87c8833cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = df[\"tags\"].apply(ast.literal_eval) # String that looks like list becomes a literal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d35e4e-f022-4ea7-a0bf-727b61ebe20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 660 unique tags!\n"
     ]
    }
   ],
   "source": [
    "def encode_tags() -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Get a list of unique tags and associate each tag with its index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], Dict[str, int]]\n",
    "        Tuple of two elements:\n",
    "            (1) tags : List[str] \n",
    "                List of unique tags\n",
    "            (2) tagToInd : Dict[str, int]\n",
    "                Dictionary that associates tags with its \n",
    "    \"\"\"\n",
    "    df_exploded = df[\"tags\"].explode() # Expand each list element into a separate row\n",
    "    \n",
    "    tags     = df_exploded.unique().tolist()                  # Extract unique values, and convert to list, for ease of use\n",
    "    tagToInd = {tag: index for index, tag in enumerate(tags)} # Associate each tag with its index\n",
    "    \n",
    "    return (tags, tagToInd)\n",
    "\n",
    "\n",
    "tags, tagToInd = encode_tags()\n",
    "print(f\"There are {len(tags)} unique tags!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6e87d3-b685-4db8-8d95-10fb44cf5264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has been successfully generated! Input shape: (9931, 3)\n",
      "Example of single input: [258 267   0]\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset() -> List[Tuple[int, int, int]]:\n",
    "    \"\"\"\n",
    "    Generate dataset to be used in training.\n",
    "    Each entry in dataset is in format (article_id, tag_id, label):\n",
    "        article_id : int\n",
    "            ID of article in whose reference we perceive the tag.\n",
    "        tag_id : int\n",
    "            ID of tag who is or is not present on article.\n",
    "        label : {1, 0}\n",
    "            If `label` is 1, then the tag is present.\n",
    "            If `label` is 0, then the tag is not present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[int, int, int]]\n",
    "        List of tuples in format (article_id, tag_id, label), as described. \n",
    "    \"\"\"\n",
    "    input = []\n",
    "\n",
    "    for idx, row in df.iterrows(): \n",
    "        tags_in_row = row[\"tags\"] # List of tags in given row\n",
    "        \n",
    "        # True data\n",
    "        for tag in tags_in_row:\n",
    "            input.append((idx, tagToInd[tag], 1)) # Every of those tags is present, therefore label them with 1\n",
    "\n",
    "        # False data - generate 5 random tags that are not present in this row\n",
    "        cnt = 0\n",
    "        while cnt != 5:\n",
    "            potential_not_present_tag = np.random.randint(0, len(tags) + 1) # Generate a singular random tag ID\n",
    "            if potential_not_present_tag not in tags_in_row: \n",
    "                input.append((idx, potential_not_present_tag, 0)) # This randomly generated tag is not present, therefore label it with 0\n",
    "                cnt += 1\n",
    "\n",
    "    input = np.random.permutation(input) # Shuffle input, just in case so the model doesn't learn irrelevant patterns\n",
    "\n",
    "    return input\n",
    "\n",
    "input = generate_dataset()\n",
    "print(f\"Input has been successfully generated! Input shape: {input.shape}\")\n",
    "print(f\"Example of single input: {input[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21eebc-9ff3-440a-8d3a-4afa0996e3fb",
   "metadata": {},
   "source": [
    "<h2> <center> 2. Model Creation </center> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88266361-2c77-4c79-80a3-49028ea07dd0",
   "metadata": {},
   "source": [
    "<h3> <center> 2.1. Model Description </center> </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d516c-22d9-4114-9603-2642079a5685",
   "metadata": {},
   "source": [
    "We aim to create a model that maps articles into an N-dimensional vector space. The closer the articles are within the space, the more similar we think they are according to some metric. Therefore, the aim is to create an <b> article embedding </b>. <br/>\n",
    "\n",
    "Embeddings contain <b> weights </b> that are learned. We are going to be creating a <b> logistic regression model </b>, i.e. <b> binary classification model </b>, that aims to fit best to our data. We will not be having training or test set, because we are not interested in the model itself, but only the learned weights that best fit to given data. Then, <b> cosine similarity </b> will be computed, and the most similar articles will be found.\n",
    "By computing the dot product among <b> article representation </b> (<b> article_embedding </b>) and <b> tag representation </b> (<b> tag_embedding </b>), we will force similarly-tagged articles to be closer to each other within N-dimensional vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24006746-60b0-4ccf-85a1-aa92e326d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleEmbeddingModel(nn.Module):\n",
    "    def __init__(self, articles_num, tags_num, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of ArticleEmbeddingModel.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - articles_num : int\n",
    "            Number of total / unique articles - each one getting its own embedding. \n",
    "        - tags_num : int \n",
    "            Number of total / unique tags - each one getting its own embedding.\n",
    "        - embedding_dim : int\n",
    "            Dimension of a singular embedding, i.e. number of dimensions used to represent a single article / tag.\n",
    "        \"\"\"\n",
    "        super(ArticleEmbeddingModel, self).__init__()\n",
    "\n",
    "        self.articles_embedding = nn.Embedding(articles_num, embedding_dim) # Embeddings of shape (articles_num x embedding_dim)\n",
    "        self.tags_embedding     = nn.Embedding(tags_num,     embedding_dim) # Embeddings of shape (tags_num     x embedding_dim)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid squashing function, used for logistic regression\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform a forward pass, on a singular (article_id, tag_id) input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tuple[torch.Tensor, torch.Tensor]\n",
    "            Input in format of (article_id, tag_id).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ...\n",
    "        \"\"\"\n",
    "        article_id, tag_id = input # Unpack tuple\n",
    "\n",
    "        try:\n",
    "            article_embedding = self.articles_embedding(article_id) # Representation of given article in embedding_dim-ensional space\n",
    "            tag_embedding     = self.tags_embedding(tag_id)         # Representation of given tag     in embedding_dim-ensional space\n",
    "        except:\n",
    "            print(\"INDEX OUT OF BOUNDS: \", article_id, tag_id)\n",
    "            \n",
    "        dot_product = torch.dot(article_embedding, tag_embedding) # Compute dot product\n",
    "\n",
    "        prediction = self.sigmoid(dot_product) # Squash the value for final prediction\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e4a3d-63cd-4613-b51e-18b75db1375b",
   "metadata": {},
   "source": [
    "<h3> <center> 2.2. Training Model </center> </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a2313-3187-4d8a-a568-127df732552b",
   "metadata": {},
   "source": [
    "Model ought to be pored to GPU (CUDA) if possible - otherwise, it's just trained on CPU. Both processes are relatively fast for given dataset size. <br/>\n",
    "Additionally, <b> criterion function </b> used is <a href=\"https://en.wikipedia.org/wiki/Cross-entropy\"> Binary Cross Entropy </a> - a standard when it comes to logistic regression tasks, while the <b> optimizer </b> is <a href=\"https://optimization.cbe.cornell.edu/index.php?title=Adam\"> Adam </a> - extended version of <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\"> Stochastic Gradient Descent </a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfe5334-eb9b-4fa0-a611-a328a53bfb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Port models and data to GPU, if possible, for faster processing\n",
    "\n",
    "articles_num  = df.shape[0] + 10 # Number of articles is the number of rows loaded\n",
    "tags_num      = len(tags) + 10   # Number of tags is the number of unique tags present \n",
    "embedding_dim = 3                # Map each article / tag in a `embedding_dim`-dimensional continuous vector space\n",
    "\n",
    "model = ArticleEmbeddingModel(articles_num, tags_num, embedding_dim).to(device) # Create model with relevant data\n",
    "\n",
    "criterion = nn.BCELoss()                            # Binary Cross-Entropy Loss is used with binary classification tasks, such as one in our case\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam ... is the best optimizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd17764-3011-4136-bdb8-d3b89c5b138c",
   "metadata": {},
   "source": [
    "Model will train for <b> epoch_num </b> epochs. Batch size is <b> 1 </b>, for conventionality reasons, and training process will inform us on average loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff85c37-a4aa-4e77-8dfa-1698f0cf9f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: #    1 Average loss: 0.9168626622551921\n",
      "Epoch: #    2 Average loss: 0.641786002538289\n",
      "Epoch: #    3 Average loss: 0.4784869268747256\n",
      "Epoch: #    4 Average loss: 0.399404675861504\n",
      "Epoch: #    5 Average loss: 0.3524544360282816\n",
      "Epoch: #    6 Average loss: 0.31811857758596607\n",
      "Epoch: #    7 Average loss: 0.29173004053455004\n",
      "Epoch: #    8 Average loss: 0.2708357645426466\n",
      "Epoch: #    9 Average loss: 0.25388813678528604\n",
      "Epoch: #   10 Average loss: 0.23916833732098916\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 10 # Self-explanatory: Number of epochs to run\n",
    "for epoch in range(epoch_num):\n",
    "    total_loss    = 0 # Total loss per epoch\n",
    "    total_batches = len(input)\n",
    "    \n",
    "    for article_id, tag_id, label in input:\n",
    "        # Turn every piece of data into a torch.Tensor\n",
    "        article_id = torch.tensor(article_id).to(device)\n",
    "        tag_id     = torch.tensor(tag_id).to(device)\n",
    "        label      = torch.tensor(label, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model((article_id, tag_id))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Accumulate total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Back-propagate\n",
    "        optimizer.zero_grad() # Zero-out the gradient\n",
    "        loss.backward()       # Back-propagate\n",
    "        optimizer.step()      # Make a step\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = total_loss / total_batches\n",
    "    print(f\"Epoch: #{epoch + 1: 5} Average loss: {average_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b39fd-5e0a-4c44-ae14-8b1715e173cf",
   "metadata": {},
   "source": [
    "<h3> <center> 2.3. Model Testing </center> </h3>\n",
    "\n",
    "As previously described, we will take <b> cosine similarity </b> to be the measure of similarity among two articles. For the sake of concept of this Notebook, we will use a <b> TEST_ARTICLE </b> and find top 5 most and least similar articles in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444dfaef-97c9-46d5-8028-e57ea59e2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for articles and topics\n",
    "article_embeddings = model.articles_embedding.weight.data.cpu().detach().numpy()\n",
    "\n",
    "# Normalize articles, so cosine similarity makes sense\n",
    "article_embeddings = article_embeddings / np.linalg.norm(article_embeddings, axis=1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ac16ec3-7e60-4cf1-babe-977847a22778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1, vec2) -> np.float32:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two embedding vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : numpy.ndarray\n",
    "        The first vector.\n",
    "    vec2 : numpy.ndarray\n",
    "        The second vector.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.float32\n",
    "        Cosine similarity between two given vectors.\n",
    "    \"\"\"\n",
    "    return cosine_similarity([vec1], [vec2])[0, 0]\n",
    "\n",
    "def find_top_similar_different_articles(target_embedding, embeddings, wanted_articles: int):\n",
    "    \"\"\"\n",
    "    Find top-X most and least similar articles, compared to target article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_embedding : numpy.ndarray\n",
    "        Target article every other article is compared to.\n",
    "    embeddings : numpy.ndarray\n",
    "        List of all article embeddings (retrieved from model).\n",
    "    wanted_articles : int\n",
    "        Number of wanted top-X articles. For example, if we wish to retrieve top-5 articles, `wanted_particles` would equal 5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Tuple[int, numpy.float32], Tuple[int, numpy.float32]]\n",
    "        Returns the tuple that contains top-X most and least similar articles.\n",
    "        Each article is represented as a tuple of (article_id, similarity).\n",
    "    \"\"\"\n",
    "    # Calculate cosine similarity for all articles\n",
    "    similarities = [calculate_cosine_similarity(target_embedding, other_embedding)\n",
    "                    for other_embedding in embeddings]\n",
    "\n",
    "    # Find top 5 most similar articles\n",
    "    top_similar_articles   = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)[:wanted_articles]\n",
    "\n",
    "    # Find top 5 least similar articles\n",
    "    top_different_articles = sorted(enumerate(similarities), key=lambda x: x[1])[:wanted_articles]\n",
    "\n",
    "    return (top_similar_articles, top_different_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce7fd6b6-a957-4b88-9559-56d757dd5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "TEST_ARTICLE    = 300 # Random article used for testing\n",
    "WANTED_ARTICLES = 5   # Number of top-X articles needed\n",
    "\n",
    "target_embedding = article_embeddings[TEST_ARTICLE]  # Use the embedding at index TEST_ARTICLE as the target embedding\n",
    "similar_articles, different_articles = find_top_similar_different_articles(target_embedding, article_embeddings, WANTED_ARTICLES)\n",
    "\n",
    "print(type(similar_articles[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0554190a-e481-4bf0-b7bb-1121649380fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article for reference:                           ['Mobile App Development', 'Mobile Apps', 'Development', 'Technology', 'Startup']\n",
      "\n",
      "Top 5 most similar articles:\n",
      "Article 300:                       1.0 --> Tags: ['Mobile App Development', 'Mobile Apps', 'Development', 'Technology', 'Startup']\n",
      "Article 960:        0.9995712041854858 --> Tags: ['Life Lessons', 'Writing', 'Creativity', 'Short Story', 'Inspiration']\n",
      "Article 657:        0.9992546439170837 --> Tags: ['Health', 'Mental Health', 'Covid 19', 'Society', 'Fitness']\n",
      "Article 650:        0.9986250996589661 --> Tags: ['Programming', 'Kubernetes', 'Microservices', 'Raspberry Pi', 'Engineering']\n",
      "Article 345:        0.9980693459510803 --> Tags: ['Happiness', 'Productivity', 'Psychology', 'Self', 'Motivation']\n",
      "\n",
      "Top 5 most different articles:\n",
      "Article 769:       -0.8886616230010986 --> Tags: ['Entrepreneurship', 'Marketing', 'Data Science', 'Data Visualization', 'Storytelling']\n",
      "Article 688:       -0.7373985648155212 --> Tags: ['Matplotlib', 'Kaggle', 'Data Analysis', 'Visualization', 'Data Science']\n",
      "Article 838:       -0.7252895832061768 --> Tags: ['Writers’ Guide', 'Tds Team', 'Writers Guide']\n",
      "Article 29:       -0.6782369017601013 --> Tags: ['Exploratory Data Analysis', 'Dashboard', 'Plotly', 'Visualization', 'Data Analysis']\n",
      "Article 771:       -0.6530123949050903 --> Tags: ['Writers’ Guide', 'Tds Team', 'Writers Guide']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Article for reference: {'':25}\", df.iloc[TEST_ARTICLE][\"tags\"])\n",
    "\n",
    "print(\"\\nTop 5 most similar articles:\")\n",
    "for article_idx, similarity in similar_articles:\n",
    "    if article_idx < 1000:\n",
    "        tags = df.iloc[article_idx]['tags']\n",
    "        print(f\"Article {article_idx}: {similarity:25} --> Tags: {tags}\")\n",
    "\n",
    "print(\"\\nTop 5 most different articles:\")\n",
    "for article_idx, similarity in different_articles:\n",
    "    if article_idx < 1000:\n",
    "        tags = df.iloc[article_idx]['tags'] \n",
    "        print(f\"Article {article_idx}: {similarity:25} --> Tags: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4891d454-88a8-4896-a608-48cab924535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"article_embedding_model.pth\") # Save model weights, in the end, so it can be used within the project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
