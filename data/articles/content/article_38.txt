A Microsoft chatbot went rogue on Twitter and started spewing Nazi epithets . It is a helpful case study in outlining some of the key issues around the application of machine learning and AI to everyday tasks. What is interesting is not that Tay, the bot, was taught to say rude things by Twitter users. What is interesting is what this tells us about designing AI systems that operate in the real world and how their learning can be hacked. Tay didn't have a sense of decent or indecent, wrong or right, should it have? And who should have taught or enforced those sensibilities? AIs are going to need to learn and interact somewhere akin to reality. Equally, if we allow AI-systems unexpurgated access to the 'real world' while they are learning, there could be ramifications. Here are some of the problems that Microsoft's Tay saga helps us explore. The first: do we have common standards for decent, ethical behaviour? If not, who draws the line? Well, we know the answer to this. We don't have common ethical standards within societies or cultures nor across them. The famous trolley problem looks at a simple moral dilemma. You see a train running down a track. There is are five people on the track who will surely die if the train continues this path. You happen to be standing by a switch that can change the path of the train. If you do that and pull the switch, the train will change tracks and hit and kill a single innocent standing on the other path. Do you pull it? This simple dilemma illustrates an ethical consideration that many AI systems will need to handle. And they will make their choice by learning their behaviour, within the parameters of their designers, or having those choice-rules explicitly programmed into them. Yes, your AI may be forced to make a choice on who to harm and who not to. The trouble is that we as humans don't agree with what to do in the Trolley problem. A recent paper looks at cultural differences and variations of the Trolley problem . It finds that ordinary British people will pull the switch, and sacrifice the one to save the five, between 63 and 91% of the time. Chinese people faced the same quandary were more inclined to let nature run its fate. They would pull the switch about 20-30% less often; or between 33% and 71% of the time. Should future systems follow British or Chinese ethical standards? And who decides? Should the designers of the systems explain how the AI is likely to perform in forced-choice situations? How do we prevent unelected, unaccountable product managers and AI programmers determining personal or social outcomes through veiled black boxes? What if those programmers are untrained in ethics, philosophy or anthropology? The second: Tay aside, we already live in a world mediated by poorly designed optimisation systems that enforce choices on hundreds of millions of people daily. Only those sysytems are less transparent and the firms operating them are often less responsive than Microsoft has been with Tay. Examples including credit scoring algorithms or complaints-handling protocols by big business. Credit scoring algorithms are often simple regressions built from small data sets relying on old information. Or in the case of complaints protocols often very simple, inflexible decision trees applied by a human. A poorly designed decision tree which might determine your access to a financial product or insurance. Getting this wrong can have real lasting impact on you and your family. How many algorithmic approaches make it out into the real world without adequate testing or understanding of their ramifications or worse, their unintended consequences? How many of these approaches are a black box, mandated monopolies, immune to competition, with limited right of redress? When we look back at Microsoft's Tay, what do we take away from it? Egg-on-the-face of a large corporation that should have known better? Absolutely. But more importantly, the AI revolutions benefits are going to come with a plethora of complexities and unintended consequences that will affect real people's real lives. Now is the time to explore them. You should totally sign-up to my Exponential View newsletter, which covers things like this. (free & awesome)